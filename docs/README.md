
# HybMIL-FSelect â€“ Pipeline de classification faiblement supervisÃ©e pour Whole Slide Images

Ce dÃ©pÃ´t regroupe lâ€™ensemble des scripts, configurations et ressources nÃ©cessaires Ã  la mise en Å“uvre de lâ€™approche HybMIL-FSelect. Cette mÃ©thode a Ã©tÃ© dÃ©veloppÃ©e dans le cadre dâ€™un mÃ©moire de Master Ã  la FacultÃ© des Sciences et Techniques de BÃ©ni Mellal (FST-BM), UniversitÃ© Sultan Moulay Slimane.

---

## 1. Objectif

Lâ€™objectif principal est de proposer une approche lÃ©gÃ¨re et interprÃ©table permettant la classification binaire de lames histologiques numÃ©riques (WSI) Ã  partir dâ€™annotations globales, dans un contexte de supervision faible. Lâ€™approche repose sur lâ€™articulation de modules de traitement visuel, de sÃ©lection de caractÃ©ristiques et dâ€™apprentissage par Multiple Instance Learning.

---

## 2. Description de lâ€™approche

HybMIL-FSelect sâ€™organise en quatre phases :

1. DÃ©coupage des images WSI en tuiles informatives Ã  lâ€™aide dâ€™OpenSlide et dâ€™un filtrage tissulaire en HSV.
2. Encodage visuel des tuiles avec le modÃ¨le prÃ©-entraÃ®nÃ© UNI, produisant des vecteurs de 1024 dimensions.
3. Filtrage stratÃ©gique des vecteurs par norme L2 et regroupement KMeans, afin de rÃ©duire le bruit visuel.
4. Classification finale par Multiple Instance Learning Ã  lâ€™aide du modÃ¨le CLAM_MB avec attention.

Lâ€™ensemble du pipeline a Ã©tÃ© exÃ©cutÃ© sur la plateforme Kaggle avec GPU T4, dans un environnement Python 3.8.

<p align="center">
  <img src="pipeline HybMILâ€‘FSelec.drawio.png" alt="Pipeline HybMIL-FSelect" width="700">
</p>

---
## 3. Structure du projet
<pre> 
â”œâ”€â”€ dataset_csv/ # Fichiers CSV dâ€™annotations 
â”œâ”€â”€ dataset_modules/ # PrÃ©traitement et loaders
â”œâ”€â”€ heatmaps/ # Cartes dâ€™attention gÃ©nÃ©rÃ©es 
â”œâ”€â”€ models/ # ImplÃ©mentation de CLAM_MB 
â”œâ”€â”€ presets/ # Fichiers de configuration 
â”œâ”€â”€ topk/ # Extraction des tuiles top-attention
â”œâ”€â”€ utils/, vis_utils/ # Fonctions utilitaires 
â”œâ”€â”€ wsi_core/ # Fonctions WSI & tiling
â”œâ”€â”€ main.py # Script dâ€™entraÃ®nement et test
â”œâ”€â”€ create_patches_fp.py # DÃ©coupage en tuiles 
â”œâ”€â”€ extract_features_fp.py # Encodage UNI 
â”œâ”€â”€ filter_features_l2_kmeans.py 
â”œâ”€â”€ create_heatmaps.py # Visualisation attention 
â”œâ”€â”€ create_splits_seq.py # K-fold splits 
â””â”€â”€ tumor_vs_normal_dummy_clean.csv  
</pre>
---

## 4. DonnÃ©es utilisÃ©es

- ğŸ“Œ **Dataset** : [SLN-Breast â€“ TCIA](https://wiki.cancerimagingarchive.net/display/Public/TCGA-BRCA)
- 130 lames `.svs` scannÃ©es Ã  20x
- Annotations binaires globales (prÃ©sence / absence de mÃ©tastases)
- DonnÃ©es organisÃ©es sous forme : `images/ + CSV (slide_id, label)`

---

## 5. Instructions de lancement

### Ã‰tape 1 : DÃ©coupage des lames

```bash
python create_patches_fp.py --source ./images --save_dir ./patches
```
<p align="center">
  <img src="decoupage de tuile.png" alt="Pipeline HybMIL-FSelect" width="700">
</p>

## Ã‰tape 2 : Encodage visuel

```bash
python extract_features_fp.py --model UNI --input ./patches --output ./features
```
## Ã‰tape 3 : Filtrage des vecteurs

```bash
python filter_features_l2_kmeans.py --input ./features --k 5
```
## Ã‰tape 4 : EntraÃ®nement CLAM_MB

```bash
python main.py --task train --config configs/hybmil.yaml
```
## Ã‰tape 5 : GÃ©nÃ©ration de cartes dâ€™attention

```bash
python create_heatmaps.py --model_path ./checkpoints/fold_1.pth
```
<p align="center">
  <img src="C3L-03262-22_0.5_roi_0_blur_0_rs_1_bc_0_a_0.4_l_1_bi_0_-1.0.jpg" alt="Cartes dâ€™attention WSI" width="650">
</p>


## 6. RÃ©sultats obtenus

| MÃ©trique   | Moyenne Â± Ã‰cart-type (10-fold CV) |
|------------|------------------------------------|
| Accuracy   | 0.933 Â± 0.086                       |
| F1-score   | 0.855 Â± 0.192                       |
| AUC        | 0.948 Â± 0.077                       |

<p align="center">
  <img src="auc_HybMIL-FSelect.png" alt="AUC PAR FOLD" width="650">
</p>


---

## 7. Configuration requise

- Python â‰¥ 3.8  
- PyTorch â‰¥ 1.10  
- OpenSlide  
- h5py  
- scikit-learn  
- pandas  

â„¹ï¸ Voir `env.yml` ou `requirements.txt`.

---

## 8. RÃ©fÃ©rences principales

- Lu et al., *CLAM: Clustering-constrained Attention MIL* (2021)  
- Chen et al., *UNI: A Universal Image Encoder for Histopathology* (2024)  
- [TCIA â€“ SLN-Breast Dataset](https://wiki.cancerimagingarchive.net/display/Public/TCGA-BRCA)

---

## 9. Auteur

**Nouhayla Skhounate**  
Master Intelligence Artificielle et Informatique Digitale  
UniversitÃ© Sultan Moulay Slimane â€“ FST BÃ©ni Mellal  
EncadrÃ©e par **Pr. Abdelali Elmoufidi**



ğŸ“„ [TÃ©lÃ©charger le mÃ©moire PDF](./docs/HybMIL-FSelect_Memoire.pdf)


